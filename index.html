<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhanyi Sun</title>

  <meta name="author" content="Zhanyi Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Zhanyi Sun
                  </p>
                  <p>I am a first-year Ph.D. student at Stanford Univeristy rotating with <a href="https://shurans.github.io//">Prof. Shuran Song</a> 
                    in the Robotics and Embodied AI (REAL) Lab. 
                  </p>
                    Prior to Stanford, I was a MS-Research student in the CMU Robotics Institute, advised by
                      <a href="https://davheld.github.io/">Prof. David Held</a> and 
                      <a href="https://zackory.com/">Prof. Zackory Erickson</a>. 
                  </p>
                  <p>Prior to CMU, I earned my B.A. in Computer Science and B.S. in Electrical Engineering 
                      from Rice University. During my time at Rice, I did research in under 
                      <a href="https://unhelkar.github.io/" target="_blank">Prof. Vaibhav Unhelkar</a>,  
                      <a href="https://www.cs.rice.edu/~kavraki/" target="_blank"> Prof. Lydia E. Kavraki</a>,  
                      and  
                      <a href="https://eiclab.scs.gatech.edu/" target="_blank"> Prof. Yingyan (Celine) Lin</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:zhanyis@stanford.edu">Email</a> &nbsp;/&nbsp;
                    <a href="data/CV_ZhanyiSun_Dec_24.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=9qap4XMAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/zhanyisun">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhanyi.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/zhanyi.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Interest</h2>
                <p>
                My research aims to develop robotic systems that 
                achieve human-like intelligence and dexterity and operate in complex and evolving environments with safety, robustness, and trustworthiness. &#129302;. 
                </p>
              </td>
            </tr>
          </tbody>
        </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publications (* indicates equal contribution)</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


      </tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='difsurvey_image'>
            <video width="200" height="200" autoplay muted>
              <source src="images/rlvlmf.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
        <td width="75%" valign="middle">
          <a href="https://rlvlmf2024.github.io/">
            <span class="papertitle">RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
            </span>
          </a>
          <br>
          <a href="https://yufeiwang63.github.io/">Yufei Wang<sup>*</sup></a>,
          <strong>Zhanyi Sun<sup>*</sup></strong>,
          <a href="https://www.jessezhang.net/">Jesse Zhang</a>, 
          <a href="https://www.zhou-xian.com/">Xian Zhou</a>, 
          <a href="https://ebiyik.github.io/">Erdem Bıyık</a>, 
          <a href="https://davheld.github.io/">David Held<sup>&#8224</sup></a>, 
          <a href="https://zackory.com/">Zackory Erickson<sup>&#8224</sup></a>
          <br>
          <em>International Conference on Machine Learning (ICML)</em>, 2024
          <br>
          <a href="https://rlvlmf2024.github.io/">Project Page</a> | 
          <a href="https://arxiv.org/abs/2402.03681">Paper</a>
          <p>We introduce a new method a method that automatically generates reward functions for agents to learn new tasks, 
            using only a text description of the task goal and the agent’s visual observations, 
            by leveraging feedback from vision language foundation models (VLMs).</p>
        </td>
      </tr>

      <tr style="height: 15px;"> <!-- Adjust the height as needed -->
        <td colspan="2"></td> <!-- This ensures the empty row spans both columns -->
      </tr>
      
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/fcvp2.png" alt="fcvp" width="200" height="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://sites.google.com/view/dressing-fcvp">
            <span class="papertitle">Force Constrained Visual Policy: Safe Robot-Assisted Dressing via
              Multi-Modal Sensing</span>
          </a>
          <br>
          <strong>Zhanyi Sun<sup>*</sup></strong>,
          <a href="https://yufeiwang63.github.io/">Yufei Wang<sup>*</sup></a>,
          <a href="https://davheld.github.io/">David Held<sup>&#8224</sup></a>,
          <a href="https://zackory.com/">Zackory Erickson<sup>&#8224</sup></a>
          <br>
          <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2024
          <br>
          <a href="https://sites.google.com/view/dressing-fcvp">Project Page</a> | 
          <a href="https://arxiv.org/abs/2311.04390">Paper</a>
          <p>We introduce a new method that leverages both vision and force modalities for robot-assisted
            dressing. Our method combines the vision-based policy,trained in simulation, with the force dynamics
            model, learned in the real world to achieve better dressing performance and safety for the user.</p>
        </td>
      </tr>

      <tr style="height: 15px;"> <!-- Adjust the height as needed -->
        <td colspan="2"></td> <!-- This ensures the empty row spans both columns -->
      </tr>
      
      </tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='difsurvey_image'>
            <video width="200" height="200" autoplay muted>
              <source src="images/one_policy.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </td>
        <td width="75%" valign="middle">
          <a href="https://sites.google.com/view/one-policy-dress/">
            <span class="papertitle">One Policy to Dress Them All: Learning to Dress People with Diverse Poses and
              Garments</span>
          </a>
          <br>
          <a href="https://yufeiwang63.github.io/">Yufei Wang</a>,
          <strong>Zhanyi Sun</strong>,
          <a href="https://zackory.com/">Zackory Erickson<sup>*</sup></a>,
          <a href="https://davheld.github.io/">David Held<sup>*</sup></a>
          <br>
          <em>Robotics: Science and Systems (RSS)</em>, 2023
          <br>
          <a href="https://sites.google.com/view/one-policy-dress/">Project Page</a> | 
          <a href="https://arxiv.org/abs/2306.12372">Paper</a> | 
          <a href="https://www.youtube.com/watch?v=5j3a0dR22oU">Video</a> | 
          <a href="https://www.cs.cmu.edu/news/2023/robot-assisted-dressing">CMU Research Highlights</a>
          <p>We develop, for the first time, a robot-assisted dressing system that is able to dress different garments on
            people with diverse body shapes and poses from partial point cloud observations, 
            based on a single Reinforcement Learning policy.</p>
        </td>
      </tr>

      <tr style="height: 20px;"> <!-- Adjust the height as needed -->
        <td colspan="2"></td> <!-- This ensures the empty row spans both columns -->
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/vitcod.png" alt="vitcod" width="200" height="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2210.09573">
            <span class="papertitle">ViTCoD: Vision transformer acceleration via dedicated algorithm and accelerator co-design</span>
          </a>
          <br>
          <a href="https://www.haoranyou.com/">Haoran You</a>,
          <strong>Zhanyi Sun</strong>,
          <a href="https://scholar.google.com/citations?user=j7AChUYAAAAJ&hl=zh-CN">Huihong Shi</a>,
          <a href="https://scholar.google.com/citations?user=KjvcaBQAAAAJ&hl=en">Zhongzhi Yu</a>,
          <a href="https://scholar.google.com/citations?user=HxeTq4MAAAAJ&hl=en">Yang Zhao</a>,
          <a href="https://scholar.google.com/citations?user=s3Qbrl0AAAAJ&hl=en">Yongan Zhang</a>,
          <a href="https://scholar.google.com/citations?user=71CuCoEAAAAJ&hl=en">Chaojian Li</a>,
          <a href="">Baopu Li</a>,
          <a href="https://scholar.google.com/citations?user=dio8IesAAAAJ&hl=en">Yingyan Lin</a>
          <br>
          <em>IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2210.09573">Paper</a> | 
          <a href="https://github.com/GATECH-EIC/ViTCoD">Code</a> | 
          <a href="https://www.youtube.com/watch?v=Foyuctc8aPE">Video</a>
          <p>We propose a dedicated algorithm and accelerator co-design framework dubbed ViTCoD to accelerate ViTs. 
            On the algorithm level, we prune and polarize the attention maps to have either denser or sparser patterns.
            On the hardware level, we develop an accelerator to coordinate the denser/sparser workloads for higeher hardware utilization.</p>
        </td>
      </tr>

      <tr style="height: 25px;"> <!-- Adjust the height as needed -->
        <td colspan="2"></td> <!-- This ensures the empty row spans both columns -->
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/supertickets.png" alt="supertickets" width="200" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2207.03677">
            <span class="papertitle">SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning</span>
          </a>
          <br>
          <a href="https://www.haoranyou.com/">Haoran You</a>,
          <a href="">Baopu Li</a>,
          <strong>Zhanyi Sun</strong>,
          <a href="https://scholar.google.com/citations?hl=en&user=IrM4gpwAAAAJ">Xu Ouyang</a>,
          <a href="https://scholar.google.com/citations?user=dio8IesAAAAJ&hl=en">Yingyan Lin</a>
          <br>
          <em>European Conference on Computer Vision (ECCV)</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2207.03677">Paper</a> | 
          <a href="https://github.com/GATECH-EIC/SuperTickets">Code</a> | 
          <a href="https://www.youtube.com/watch?v=HTlaew_6Vmg">Video</a>
          <p>We discover for the first time that both efficient DNNs and their lottery subnetworks can be identified from a supernet and propose 
            a two-in-one training scheme with jointly architecture searching and parameter pruning to identify them.</p>
        </td>
      </tr>

    <tr style="height: 25px;"> <!-- Adjust the height as needed -->
      <td colspan="2"></td> <!-- This ensures the empty row spans both columns -->
    </tr>
      
    </tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='difsurvey_image'>
          <video width="200" height="200" autoplay muted>
            <source src="images/blind.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </td>
      <td width="75%" valign="middle">
        <a href="https://carlosquinterop.github.io/project/blind/">
          <span class="papertitle">Human-guided motion planning in partially observable environments</span>
        </a>
        <br>
        <a href="https://carlosquinterop.github.io/">Carlos Quintero-Pena<sup>*</sup></a>,
        <a href="https://cchamzas.com/">Constantinos Chamzas<sup>*</sup></a>,
        <strong>Zhanyi Sun</strong>,
        <a href="https://unhelkar.github.io/">Vaibhav Unhelkar</a>,
        <a href="https://www.cs.rice.edu/~kavraki/">Lydia E. Kavraki</a>
        <br>
        <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
        <br>
        <a href="https://carlosquinterop.github.io/project/blind/">Project Page</a> | 
        <a href="https://par.nsf.gov/servlets/purl/10359068">Paper</a> | 
        <a href="https://www.youtube.com/watch?v=m1rc8JNBMAQ">Video</a> | 
        <a href="https://www.futurity.org/robot-human-interaction-motion-algorithms-2755042-2/">Futurity Research News Review</a>
        <p>We propose a method that leverages human guidance for high DOF robot motion planning in partial observable
          environments. We project the robot’s continuous configuration space to a discrete task model and utilize 
          inverse RL to learn motion-level guidance from human critiques.</p>
      </td>
    </tr>

      
    </tbody>
  </table>


  <!-- Education -->

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Education</h2>
        </td>
      </tr>
    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr>
        <td style="padding:10px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/cmu_logo.png' width="120">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:top">
          <papertitle><big> Carnegie Mellon University </big></papertitle>
          <br>
          <papertitle style="color:gray"><big>Master of Science in Robotics (MSR)</big> </papertitle>
          <br>
          <br>
          Aug '22 - Jun '24'
          <br>
          <br>
          <p> Advised <a href="https://davheld.github.io/" target="_blank">Prof. David Held</a> and <a
              href="https://zackory.com/" target="_blank"> Prof. Zackory Erickson</a>.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:10px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/rice_logo.jpeg' width="120">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:top">
          <papertitle><big> Rice University </big></papertitle>
          <br>
          <papertitle style="color:gray"><big>B.A. in Computer Science, B.S. in Electrical Engineering</big>
          </papertitle>
          <br>
          <br>
          Aug '18 - May '22
          <br>
          <br>
          <p> Advised <a href="https://unhelkar.github.io/" target="_blank">Prof. Vaibhav Unhelkar</a>, <a
              href="https://www.cs.rice.edu/~kavraki/" target="_blank"> Prof. Lydia E. Kavraki</a>, and <a
              href="https://eiclab.scs.gatech.edu/" target="_blank"> Prof. Yingyan (Celine) Lin</a></p>
        </td>
      </tr>


    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron </a>.
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>